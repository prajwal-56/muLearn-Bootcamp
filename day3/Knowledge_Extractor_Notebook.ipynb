{"cells":[{"cell_type":"markdown","id":"97541bc9","metadata":{"id":"97541bc9"},"source":["# Day 3 – Multi‑Agent Long‑Text Knowledge Extractor (Gemini‑style)\n","\n","A compact multi-agent notebook that demonstrates chunking, summarization, and synthesis for long texts. This notebook mirrors the structure of your bootcamp example but implements a working 'Knowledge Extractor' pipeline.\n","\n","**Agents:** ChunkerAgent, SummarizerAgent, SynthesizerAgent, ValidationAgent (optional).\n","\n","You can run this locally or in Colab. If you have a Gemini / Google Cloud Vertex AI API key, set it in the configuration cell below to use the Gemini model. Otherwise the notebook uses a lightweight local summarizer fallback."]},{"cell_type":"markdown","id":"e868c07f","metadata":{"id":"e868c07f"},"source":["## Install dependencies\n","\n"]},{"cell_type":"code","execution_count":1,"id":"3f3751f3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3f3751f3","executionInfo":{"status":"ok","timestamp":1765301796605,"user_tz":-330,"elapsed":11448,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}},"outputId":"3471ab9b-4e77-47fe-e590-38f433dcf0cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/475.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m471.0/475.3 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q google-genai langchain-google-genai langchain"]},{"cell_type":"markdown","id":"3efe06a7","metadata":{"id":"3efe06a7"},"source":["## 2. Configure Gemini API Key"]},{"cell_type":"code","execution_count":2,"id":"36854b45","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36854b45","executionInfo":{"status":"ok","timestamp":1765301845506,"user_tz":-330,"elapsed":192,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}},"outputId":"c5e57b6a-a952-4030-c879-c6d72b65a0d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["GOOGLE_API_KEY set: False\n"]}],"source":["\n","import os\n","# Example (uncomment + replace to use):\n","# os.environ['GOOGLE_API_KEY'] = \"YOUR_GOOGLE_API_KEY\"\n","\n","print(\"GOOGLE_API_KEY set:\", 'GOOGLE_API_KEY' in os.environ)\n"]},{"cell_type":"markdown","id":"5db572f9","metadata":{"id":"5db572f9"},"source":["## 3. This notebook implements a three-stage multi-agent pipeline:\n","\n","1. **ChunkerAgent** — Split the long text into manageable chunks.\n","2. **SummarizerAgent** — Summarize each chunk (calls Gemini if available, else uses local heuristic summarizer).\n","3. **SynthesizerAgent** — Merge chunk summaries into a single coherent knowledge digest.\n","\n","There's also a **ValidationAgent** to do simple checks on the final summary."]},{"cell_type":"markdown","id":"ed878a31","metadata":{"id":"ed878a31"},"source":["### ChunkerAgent\n","Splits the text into chunks by characters or sentences, with optional overlap."]},{"cell_type":"code","execution_count":8,"id":"a40783ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a40783ef","executionInfo":{"status":"ok","timestamp":1765301988352,"user_tz":-330,"elapsed":6,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}},"outputId":"6f5da64a-c871-49bd-e73b-74b46c221c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chunks: 11\n"]}],"source":["\n","from typing import List\n","import re\n","\n","def chunk_text(text: str, max_chars: int = 1500, overlap: int = 200) -> List[str]:\n","    \"\"\"Split text into chunks of max_chars with simple sentence-aware boundaries where possible.\"\"\"\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    if len(text) <= max_chars:\n","        return [text]\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + max_chars\n","        if end >= len(text):\n","            chunks.append(text[start:])\n","            break\n","        snippet = text[start:end]\n","        last_period = snippet.rfind('. ')\n","        if last_period != -1 and last_period > int(max_chars*0.5):\n","            cut = start + last_period + 1\n","        else:\n","            cut = end\n","        chunks.append(text[start:cut].strip())\n","        start = max(cut - overlap, cut)\n","    return chunks\n","\n","# quick sanity check\n","if __name__ == '__main__':\n","    sample = \"Lorem Ipsum Dolor Sit amen.\" * 80\n","    print('Chunks:', len(chunk_text(sample, max_chars=200, overlap=40)))\n"]},{"cell_type":"markdown","id":"3f1c328b","metadata":{"id":"3f1c328b"},"source":["### SummarizerAgent\n","Attempts to use Gemini if configured; otherwise falls back to a lightweight extractive summarizer.\n","\n","The Gemini call is shown as a template — uncomment and configure your credentials to enable it."]},{"cell_type":"code","execution_count":11,"id":"710f62d4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"710f62d4","executionInfo":{"status":"ok","timestamp":1765302272316,"user_tz":-330,"elapsed":29,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}},"outputId":"2ef9e4c8-98d9-4070-f0e6-c42937dc8369"},"outputs":[{"output_type":"stream","name":"stdout","text":["As the large language models powering generative AI tools slurp up ever more data across the web, Cloudflare cofounder and CEO Matthew Prince said at WIRED’s Big Interview event in San Francisco on Thursday that the internet infrastructure company has blocked more than 400 billion AI bot requests for its customers since July 1. The action comes after the company announced a Content Independence Day in July—an initiative with prominent publishers and AI firms to block AI crawlers by default on content creators’ work unless the AI companies pay for access. Cloudflare told WIRED that the number of AI bots blocked since July 1, 2025 is 416 billion.\n"]}],"source":["\n","def local_extractive_summarizer(text: str, max_sentences: int = 5) -> str:\n","    \"\"\"A tiny extractive summarizer: score sentences by term frequency and pick top ones.\"\"\"\n","    import math, re\n","    from collections import Counter\n","\n","    sentences = re.split(r'(?<=[.!?])\\s+', text)\n","    if len(sentences) <= max_sentences:\n","        return ' '.join(sentences).strip()\n","    words = re.findall(r\"\\w+\", text.lower())\n","    freq = Counter(words)\n","    scores = []\n","    for i, s in enumerate(sentences):\n","        ws = re.findall(r\"\\w+\", s.lower())\n","        if not ws:\n","            scores.append((i, 0.0))\n","            continue\n","        score = sum(freq[w] for w in ws) / math.sqrt(len(ws))\n","        scores.append((i, score))\n","    scores.sort(key=lambda x: x[1], reverse=True)\n","    top_idx = sorted([i for i,_ in scores[:max_sentences]])\n","    summary = ' '.join([sentences[i] for i in top_idx])\n","    return summary.strip()\n","\n","def summarize_chunk(chunk: str, use_gemini: bool = False, gemini_client=None, max_sentences: int = 5) -> str:\n","    if use_gemini and gemini_client is not None:\n","        # Example pseudo-call for Gemini (uncomment and adapt to real client)\n","        # prompt = f\"\"\"Summarize the following text concisely, preserving key ideas and important details.\\n\\n{chunk}\\n\\nCONCISE SUMMARY:\"\"\"\n","        # response = gemini_client.generate_text(prompt)  # pseudocode\n","        # return response.text\n","        raise NotImplementedError(\"Gemini client integration is left as an exercise; set use_gemini=False to use fallback.\")\n","    else:\n","        return local_extractive_summarizer(chunk, max_sentences=max_sentences)\n","\n","# demo\n","if __name__ == '__main__':\n","    t = (\"\"\"As the large language models powering generative AI tools slurp up ever more data across the web,\n","    Cloudflare cofounder and CEO Matthew Prince said at WIRED’s Big Interview event in San Francisco on Thursday that the internet infrastructure company has blocked more than 400 billion AI bot requests for its customers since July 1.\n","\n","The action comes after the company announced a Content Independence Day in July—an initiative with prominent publishers\n","and AI firms to block AI crawlers by default on content creators’ work unless the AI companies pay for access.\n","Since July 2024, Cloudflare has offered customers tools to block AI bots from scraping their content.\n","Cloudflare told WIRED that the number of AI bots blocked since July 1, 2025 is 416 billion.\n","\n","\"\"\")\n","    print(local_extractive_summarizer(t, max_sentences=3))\n"]},{"cell_type":"code","execution_count":12,"id":"8284fef8","metadata":{"id":"8284fef8","executionInfo":{"status":"ok","timestamp":1765302328817,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}}},"outputs":[],"source":["\n","from typing import List\n","def summarize_all(chunks: List[str], use_gemini: bool = False, gemini_client=None, max_sentences: int = 5) -> List[str]:\n","    summaries = []\n","    for i, c in enumerate(chunks):\n","        s = summarize_chunk(c, use_gemini=use_gemini, gemini_client=gemini_client, max_sentences=max_sentences)\n","        summaries.append(s)\n","        print(f\"Summarized chunk {i+1}/{len(chunks)} — {len(c)} chars -> {len(s)} chars\")\n","    return summaries\n"]},{"cell_type":"markdown","id":"2fec87e6","metadata":{"id":"2fec87e6"},"source":["### SynthesizerAgent\n","Takes the chunk summaries and merges them into a single coherent summary. Optionally asks Gemini to polish the final summary."]},{"cell_type":"code","execution_count":15,"id":"29c5baf4","metadata":{"id":"29c5baf4","executionInfo":{"status":"ok","timestamp":1765303300996,"user_tz":-330,"elapsed":4,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}}},"outputs":[],"source":["\n","def synthesize_summaries(summaries: List[str], use_gemini: bool = False, gemini_client=None) -> str:\n","    joined = \"\\n\\n\".join(summaries)\n","    if use_gemini and gemini_client is not None:\n","        # pseudo code for Gemini polishing\n","        prompt = f\"\"\"You are an AI assistant. Combine the following chunk summaries into one coherent, structured summary. Use bullets or short paragraphs and highlight key insights.\\n\\n{joined}\\n\\nFINAL SUMMARY:\"\"\"\n","        response = gemini_client.generate_text(prompt)\n","        return response.text\n","        # raise NotImplementedError(\"Gemini polishing not implemented in this template.\")\n","    import re\n","    sentences = re.split(r'(?<=[.!?])\\s+', joined)\n","    seen = set()\n","    out = []\n","    for s in sentences:\n","        key = s.strip().lower()\n","        if not key or key in seen:\n","            continue\n","        seen.add(key)\n","        out.append(s.strip())\n","    final = ' '.join(out[:10])\n","    return final.strip()\n"]},{"cell_type":"markdown","id":"29b96c66","metadata":{"id":"29b96c66"},"source":["### ValidationAgent\n","Simple checks to ensure the summary length and that it contains keywords from the original text."]},{"cell_type":"code","execution_count":16,"id":"a54c9d77","metadata":{"id":"a54c9d77","executionInfo":{"status":"ok","timestamp":1765303306897,"user_tz":-330,"elapsed":2,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}}},"outputs":[],"source":["\n","from typing import Tuple\n","def validate_summary(original: str, summary: str, min_coverage_ratio: float = 0.05) -> Tuple[bool, dict]:\n","    import re\n","    orig_words = set(re.findall(r\"\\w+\", original.lower()))\n","    summ_words = set(re.findall(r\"\\w+\", summary.lower()))\n","    if not orig_words:\n","        return False, {'reason': 'original empty'}\n","    coverage = len(summ_words & orig_words) / len(orig_words)\n","    ok = coverage >= min_coverage_ratio\n","    return ok, {'coverage': coverage, 'summary_len': len(summary)}\n"]},{"cell_type":"markdown","id":"6b03b123","metadata":{"id":"6b03b123"},"source":["## 4. Orchestrator / Run the pipeline\n","This cell demonstrates running all agents together on a sample long text. Replace `sample_text` with your document or load a `.txt` file."]},{"cell_type":"code","execution_count":17,"id":"b29b06cf","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b29b06cf","executionInfo":{"status":"ok","timestamp":1765303314423,"user_tz":-330,"elapsed":10,"user":{"displayName":"Prajwal","userId":"02032751182433096958"}},"outputId":"fcf6961e-6bd7-4e36-9be2-536dc03a1e38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Chunking...\n","Created 7 chunks.\n","Summarized chunk 1/7 — 787 chars -> 553 chars\n","Summarized chunk 2/7 — 723 chars -> 489 chars\n","Summarized chunk 3/7 — 774 chars -> 591 chars\n","Summarized chunk 4/7 — 716 chars -> 475 chars\n","Summarized chunk 5/7 — 628 chars -> 540 chars\n","Summarized chunk 6/7 — 628 chars -> 540 chars\n","Summarized chunk 7/7 — 771 chars -> 587 chars\n","\n","Synthesizing final summary...\n","\n","Validation: True {'coverage': 1.0, 'summary_len': 929}\n","\n","FINAL SUMMARY:\n"," Deep learning is a subset of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. The field has grown rapidly in the last decade, driven by cheaper compute and larger datasets. It also serves as a good demo for agentic systems where small specialized 'agents' perform isolated tasks and pass results to the next agent.Deep learning is a subset of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. This notebook demonstrates how to split long documents into chunks, summarize each piece, and then recombine the results into a coherent summary. The approach is robust for long blog posts, technical reports, and meeting transcripts. It also serves as a good demo for agentic systems where small specialized 'agents' perform isolated tasks and pass results to the next agent.\n"]}],"source":["\n","# Example orchestration\n","def run_pipeline(text: str, max_chars: int = 1500, overlap: int = 200, max_sentences: int = 5, use_gemini: bool = False, gemini_client=None):\n","    print('Chunking...')\n","    chunks = chunk_text(text, max_chars=max_chars, overlap=overlap)\n","    print(f'Created {len(chunks)} chunks.')\n","    summaries = summarize_all(chunks, use_gemini=use_gemini, gemini_client=gemini_client, max_sentences=max_sentences)\n","    print('\\nSynthesizing final summary...')\n","    final = synthesize_summaries(summaries, use_gemini=use_gemini, gemini_client=gemini_client)\n","    ok, info = validate_summary(text, final)\n","    print('\\nValidation:', ok, info)\n","    return {'chunks': chunks, 'summaries': summaries, 'final_summary': final}\n","\n","# Load sample text (or replace with your own)\n","sample_text = \"\"\"Deep learning is a subset of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. The field has grown rapidly in the last decade, driven by cheaper compute and larger datasets. This notebook demonstrates how to split long documents into chunks, summarize each piece, and then recombine the results into a coherent summary. The approach is robust for long blog posts, technical reports, and meeting transcripts. It also serves as a good demo for agentic systems where small specialized 'agents' perform isolated tasks and pass results to the next agent.\"\"\" * 8\n","\n","result = run_pipeline(sample_text, max_chars=800, overlap=120, max_sentences=3)\n","print('\\nFINAL SUMMARY:\\n', result['final_summary'])\n"]},{"cell_type":"code","source":[],"metadata":{"id":"7xFMpMjlRuaU"},"id":"7xFMpMjlRuaU","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}